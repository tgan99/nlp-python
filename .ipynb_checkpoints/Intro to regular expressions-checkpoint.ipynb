{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1:  Introduction to NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Regular expressions & word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing regular expressions: re.split() and re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Intro to tokenization:\n",
    "\n",
    "## Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "file = open('datasets/monty_python_holy_grail.txt', mode='r')\n",
    "\n",
    "# Print it\n",
    "# print(file.read())\n",
    "scene_one = file.read()\n",
    "\n",
    "# Close file\n",
    "file.close()\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tedeagan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8', 'Amen', 'never', 'averting', 'chorus', 'much', 'middle', 'knees-bent', 'cross', 'My', 'Enchanter', 'nightfall', 'night', 'Joseph', 'clue', 'OLD', '.', 'weight', 'enchanter', 'shit', 'awfully', 'Could', 'Riiight', 'dynamite', 'glass', 'up', 'Heee', 'went', \"d'you\", 'ere', 'lives', 'lovely', 'Say', 'foot', 'Once', 'Dappy', 'smack', 'discovered', 'heart', 'government', 'marry', 'repressed', 'me', 'little', 'Herbert', 'Recently', 'problem', 'visually', 'herring', 'feint', 'trumpets', 'brunettes', 'temptation', 'seldom', 'squeak', 'FATHER', 'Lie', 'bless', 'trusty', 'Apples', 'hundred-and-fifty', 'Very', 'remain', 'VILLAGER', 'in', 'second', 'Pure', 'enjoying', 'snap', 'coconut', 'either', 'stuffed', 'Hallo', 'Armaments', 'lucky', 'heard', 'self-perpetuating', 'fourth', 'Thppt', 'last', 'pray', 'cruel', 'presence', 'illegitimate-faced', 'PARTY', 'Waa', 'Other', 'God', 'Not-appearing-in-this-film', 'Winston', 'HERBERT', 'later', 'For', 'What', 'outdated', 'apart', 'Robinson', 'Looks', 'yel', 'hopeless', 'jokes', 'nearer', 'song', 'says', 'horn', 'AMAZING', 'Oh', 'diaphragm', 'ladies', 'ours', 'broken', 'fine', 'retold', 'verses', 'pound', 'major', 'PRISONER', 'Between', 'She', 'resting', 'mad', 'Table', 'jump', 'five', 'call', 'Meanwhile', 'forth', 'exciting', 'aloft', 'imprisoned', 'sons', 'Five', 'warm', 'identical', 'Off', 'dirty', 'la', 'daughter', 'ran', 'it', 'tale', 'getting', 'chickened', 'pestilence', 'living', 'everyone', 'brush', 'ha..', 'outdoors', 'One', 'chance', 'chanting', 'yourself', 'clap', 'biggest', 'eisrequiem', 'Hm', 'looking', 'while', 'rewr', 'tired', 'miserable', 'want', 'Chicken', 'amazes', 'risk', 'harmless', 'tell', 'eyes', 'dying', 'logically', 'pause', 'zone', 'kill', 'fatal', 'sloths', 'His', 'fair', 'oo', 'yes', 'Zoot', 'pond', 'nice-a', 'Camelot', 'examine', 'Ay', 'ai', 'scots', 'Quoi', 'Yay', 'CRASH', 'fruit', 'anyway', 'distress', 'burst', 'handle', 'LEFT', 'gra', 'properly', 'upon', \"'cause\", 'passed', 'might', 'ROGER', 'Said', 'valiant', 'bi-weekly', 'off', 'quiet', 'dragging', 'assault', 'Thou', 'Yapping', 'wood', 'object', 'siren', 'yeah', 'looney', 'sonny', 'case', 'frighten', 'man', 'argue', 'saw', 'young', 'sharp', 'MINSTREL', 'They', 'cop', 'guarded', 'fortune', \"'anging\", 'string', 'union', 'FRENCH', 'Ridden', 'frozen', 'bois', 'our', 'liege', 'On', 'wet', 'clear', 'thine', 'arms', 'WITCH', 'Pull', 'parts', '?', 'Thpppt', 'awaaaaay', 'splash', 'south', 'rodent', 'KNIGHTS', 'only', 'adversary', 'Grenade', 'gurgle', 'Quickly', 'Shall', 'lie', 'Bones', 'bastard', 'pig-dogs', 'Winter', 'shivering', 'problems', 'Aauuggghhh', 'task', 'lad', 'bells', 'feet', 'answer', 'eis', 'elbows', 'influential', 'scimitar', 'well', 'eet', 'higher', 'un', 'progress', '9', 'returns', 'Knight', 'knight', 'scratch', 'Ask', 'personally', 'leads', 'covered', 'follow', 'fallen', 'talk', 'liver', 'grip', 'ungallant', 'entering', 'bang', \"'Dennis\", 'holy', \"'it\", 'wooden', 'Great', 'understand', 'awaits', 'Skip', 'de', 'scales', 'sacred', 'Aaaah', 'bold', \"'aaaah\", 'today', 'How', 'buggering', 'Charge', 'elderberries', 'Yeaaah', 'acting', 'SIR', 'favor', 'spank', 'help', 'Wood', 'bint', 'tit', 'banana-shaped', 'swallow', 'gouged', 'made', 'Aauuugh', 'bitching', 'keepers', 'Shrubberies', 'plain', 'I', 'strange', 'Quite', 'See', 'her', 'splat', 'Whoa', 'Eternal', 'Chapter', 'body', 'keeper', 'utterly', 'enough', 'Must', 'send', 'way', 'Aramaic', 'fire', 'tie', 'vote', 'Black', 'supreme', 'PIGLET', 'without', 'score', 'always', 'ask', '--', 'Hold', 'mercy', 'hang', 'filth', 'ounce', 'reared', 'grin', 'questions', 'bugger-folk', 'Welcome', 'like', 'aunties', 'French', 'died', 'Dramatically', 'sun', 'dungeon', 'watery', 'dear', 'tough', 'brain', 'Is', 'LAUNCELOT', 'happens', 'are', 'number', 'wounded', 'BRIDE', 'hall', 'dramatic', 'moistened', 'Who', 'autonomous', 'convinced', 'him', 'high-pitched', 'rest', 'cereals', 'bleed', 'they', 'Antioch', 'behaviour', 'U', 'pulp', 'mate', 'k-nnnnniggets', 'surprise', 'business', 'Frank', 'Stop', 'Do', 'minute', 'Will', 'awhile', 'mystic', 'hand', 'Throw', 'cave', 'lair', 'Anyway', 'Nador', 'cover', 'bones', 'raised', 'change', 'Ewing', 'wart', 'lord', 'And', 'blow', 'twenty', 'armed', 'pounds', 'makes', 'winter', 'explain', 'nasty', 'medical', 'police', 'thwonk', 'approaching', 'Really', 'SOLDIER', 'times', 'halves', 'inherent', 'two', \"'Aauuuuugh\", 'fight', 'here', 'weapon', 'domine', 'Bedevere', 'earthquakes', 'Run', 'Supposing', 'outrageous', '10', 'Book', 'carried', 'Try', 'NARRATOR', 'stab', 'such', 'buggered', 'In', 'mangled', 'very', 'Wayy', 'jam', 'done', 'formidable', 'bladders', 'anywhere', 'Swamp', 'Hiyah', 'Heh', 'When', 'blondes', 'late', 'MAN', 'Gorge', 'marrying', 'It', 'anyone', 'Lucky', 'LUCKY', 'drilllll', 'ca', 'Your', 'Shh', 'allowed', 'himself', 'knights', 'get', \"'S\", 'suffice', 'quest', 'almost', \"'Erbert\", 'scholar', 'assist', 'bleeder', 'north', 'singing', 'oh', 'eight', 'pass', 'word', 'PERSON', 'ratified', 'vache', 'think', 'son', 'smashing', ')', 'This', 'Augh', 'tragic', 'mangy', 'bird', 'ethereal', 'carries', 'keep', 'window-dresser', 'avenged', 'using', 'rescue', 'north-east', 'van', 'mud', 'BRIDGEKEEPER', 'Midget', 'pull', 'bed-wetting', 'straight', 'violence', '6', 'SUN', 'occasion', 'emperor', 'Lead', 'magne', 'Too', 'cope', \"'ni\", '23', '[', 'crossed', 'sense', 'example', 'resumes', 'remember', 'thonk', 'manner', \"'Ni\", 'No', '16', 'proceed', 'got', 'indeed', 'cut', 'Aaauggh', 'recover', 'snows', 'grovel', 'rope', 'end', 'Tale', 'Of', 'Brave', 'see', 'Steady', 'KING', 'use', 'Most', 'Silly', 'started', 'auuuuuuuugh', 'forward', 'shows', 'blessing', 'Hoa', 'too', 'walking', 'come', 'whose', 'clllank', 'Splendid', 'proved', 'Erm', 'WIFE', 'Attila', 'fly', 'p', 'ever', 'or', 'merger', 'bridgekeeper', 'feathers', 'Clark', ':', 'busy', 'OTHER', 'Have', 'saved', 'impeccable', 'oral', 'Bad', 'duty', 'mother', 'more', 'water', 'delirious', 'Cornwall', 'Why', 'English', 'lapin', 'CRONE', 'us', 'RIGHT', 'differences', 'NI', 'wedlock', 'good', 'Tower', 'wonderful', 'certainly', 'France', 'any', 'Here', 'he', 'King', 'hear', 'some', 'Bridge', 'heh..', 'Uuh', 'Like', 'Farewell', \"C'est\", 'afoot', 'things', 'friend', '2', '14', 'knows', 'N', 'side', 'beacon', 'guided', 'general', 'Hill', 'executive', 'who', 'vary', 'pig', 'Hang', 'bits', 'having', \"'is\", 'velocity', 'Ho', 'scene', 'MAYNARD', 'CAMERAMAN', 'pure', 'strategy', 'Shut', 'hell', 'glory', 'completely', 'bottoms', 'Am', 'shall', 'aptly', 'MIDGET', 'Hya', 'kings', 'lived', 'havin', 'consulted', \"'ve\", '...', 'teeth', 'power', 'everything', 'hmm', 'go', 'Those', '19', 'Bloody', 'Away', 'scrape', 'huge', 'knocked', 'cartoon', 'with', 'Running', 'Consult', 'Guy', 'yet', 'bad-tempered', 'throat', 'lobbed', 'fart', 'ugly', 'horse', 'court', 'near', 'pansy', 'Allo', 'Piglet', 'varletesses', 'Ninepence', 'Stay', 'a', 'That', 'wipers', 'bid', 'Christ', 'knock', \"'Morning\", 'helpful', 'Thursday', 'creature', 'act', 'bit', 'bum', 'scared', 'Idiom', 'longer', 'dad', 'leave', 'illustrious', 'bottom', 'deeds', 'lobbest', 'voluntarily', 'joyful', 'dressed', 'closest', 'Lancelot', 'witness', 'unclog', 'maintain', 'changed', 'Father', 'Hey', 'Un', 'your', 'show', 'crone', 'Nu', 'Monsieur', 'mistake', 'collective', '22', 'ordinary', '7', 'ones', 'e', 'ho', 'Unfortunately', 'stayed', 'brave', 'drink', 'should', 'thou', 'Ulk', 'beat', 'Good', 'push', 'Cut', 'Aaaaaah', 'imperialist', 'you', 'chest', 'Bedwere', 'test', 'beyond', 'Make', 'freedom', 'O', 'Action', 'used', 'Caerbannog', 'nothing', 'haste', 'class', 'kind', 'conclusions', 'thy', 'wants', 'beds', 'rode', 'Pie', 'new', 'Picture', 'c', 'mortally', 'fellows', 'time', 'four', 'Roger', 'mine', 'biters', 'traveller', 'May', 'easy', 'required', 'an', 'WINSTON', 'whop', 'coming', 'Princess', 'week', 'BLACK', 'opera', 'um', 'Hmm', 'comin', 'wiper', 'head', 'built', 'town', 'internal', 'absolutely', 'Does', 'down', 'dunno', 'signifying', 'ha', 'whether', 'Lake', 'note', 'majority', 'woman', 'Jesus', 'previous', 'sight', 'DEAD', 'dare', 'Our', 'honored', 'bicker', 'Mmm', \"'aaggggh\", 'Perhaps', 'Come', 'exploiting', 'lies', 'quarrel', 'stone', 'Lady', 'bringing', 'Anarcho-syndicalism', 'HEAD', 'wave', 'Ow', 'lying', 'excuse', 'gone', 'performance', 'headoff', 'wind', 'vain', 'chickening', 'Spring', 'wo', 'CHARACTERS', 'Until', 'Tall', 'king', 'Nay', '18', 'Grail', 'clank', 'outwit', 'eh', 'tiny-brained', 'also', 'another', 'dark', 'dappy', 'watch', 'grail-shaped', 'say', 'nibble', 'my', 'Far', 'open', 'wings', 'spake', 'best', 'horrendous', 'seems', 'Aaaugh', 'Almighty', 'door', 'defeat', 'feel', 'give', 'house', 'dogma', 'GOD', 'turns', 'hills', 'OFFICER', 'kills', 'Uh', 'ZOOT', 'Brother', 'derives', 'swallows', 'time-a', 'uuup', 'decided', 'taken', 'sing', 'Now', \"'Course\", 'depressing', 'Iesu', 'charged', 'Or', 'shut', 'maybe', 'lads', 'Honestly', 'cry', 'Loimbard', 'Umhm', 'mandate', 'mooooooo', 'eats', 'Open', 'to', 'praised', 'Galahad', 'With', 'inferior', 'laden', 'brought', 'trade', 'Gawain', 'nor', 'Gable', 'castanets', 'Man', 'Halt', 'quite', 'nostrils', 'vouchsafed', 'worked', 'spooky', 'migrate', 'together', 'set', 'Wait', 'w', 'spoken', 'yours', 'met', 'Hiyaah', 'tail', 'Let', 'supposed', 'Cider', 'awaaay', 'purpose', 'most', 'dona', 'formed', 'Practice', 'length', 'entrance', 'Saxons', 'Aaah', 'took', 'had', 'feast', \"'d\", 'into', 'strength', 'Ni', 'against', 'commands', 'return', 'Help', 'bet', \"'re\", 'Ives', 'creeper', 'u', 'seek', 'routines', 'Ooh', 'clunk', 'killer', 'Rheged', 'Back', 'compared', 'Arimathea', 'bring', 'fought', 'wait', \"'T\", 'doctors', 'peasant', 'out', 'owns', 'sister', 'profane', 'place', 'testicles', 'safety', 'build', 'real', 'i', 'worry', 'round', 'force', 'eccentric', 'have', 'Prepare', 'worst', 'An', 'England', 'Forgive', '1', 'rocks', 'economic', 'W', 'grips', 'removed', 'interested', 'GIRLS', 'the-not-quite-so-brave-as-Sir-Lancelot', 'temperate', 'moment', 'coconuts', 'idiom', 'death', 'Aauuuves', 'Concorde', 'fwump', 'Aah', 'creep', 'A', 'snuff', 'around', 'rhymes', \"'Oooooooh\", 'wield', 'witch', 'ALL', 'Cherries', 'those', 'Torment', 'Prince', 'terrible', '!', 'Maynard', 'am', 'matter', 'wise', 'travellers', 'bows', 'disheartened', 'direction', \"'shrubberies\", 'under', 'Bring', 'favorite', 'largest', 'Right', 'art', 'commune', 'so', 'Schools', 'other', 'leg', 'basis', 'apologise', '17', 'zoosh', 'how', 'guests', 'someone', 'grail', 'b', 'As', 'bowels', 'sir', 'Sir', 'tree', 'Fiends', 'hat', \"'old\", 'Auuuuuuuugh', 'flesh', 'ptoo', 'need', 'mightiest', 'taking', 'outside', 'trouble', '12', 'forty-three', 'bride', 'eat', 'remembered', 'ready', 'Badon', 'Keep', 'quack', 'band', 'wherein', '21', 'beside', 'uhh', 'mayhem', 'point', 'killed', 'from', 'long', 'suffered', 'every', 'donaeis', 'Yeah', 'slash', 'HEADS', 'Aagh', 'obviously', 'Victory', 'Hiyya', 'Everything', 'PRINCESS', 'tea', 'Seek', 'full', 'old', 'officer', 'suspenseful', 'tracts', 'Dis-mount', 'CROWD', 'science', 'plover', 'affairs', 'Hand', '4', 'ARMY', 'idea', 'shrubber', 'Britain', '15', 'still', 'mumble', 'shrubberies', 'high', 'Message', 'then', 'join', 'social', 'lost', 'daring', 'creak', 'minstrels', 'grenade', 'breakfast', 'two-thirds', 'Hello', 'woods', 'left', 'Tim', 'name', 'hamster', 'nearly', 'Doctor', 'Remove', 'girl', 'automatically', 'breath', 'KNIGHT', 'ruffians', 'chosen', 'anchovies', 'learning', 'between', 'spanked', 'preserving', 'somebody', 'year', 'Just', 'meeting', 'yellow', 'purely', 'make', 'flights', 'therefore', 'MIDDLE', 'danger', 'boil', 'Agh', 'Chickennn', 'throwing', 'dull', 'needs', 'although', 'smashed', 'society', 'bunny', 'Ah', 'samite', 'easily', 'retreat', 'soft', 'TIM', 'seen', 'Assyria', 'wounding', 'attack', 'chastity', 'table', 'why', 'evil', 'vital', 'Aaauugh', 'Britons', 'when', 'Silence', 'history', 'Tell', 'Firstly', 'going', 'turned', 'for', 'over', 'medieval', 'European', 'headed', 'gallantly', 'twang', 'daft', 'fifty', 'oui', 'silly', 'dictating', 'being', 'rejoicing', 'mer', 'But', 'DIRECTOR', 'bonk', 'really', 'could', 'relax', 'must', 'Quick', 'Four', 'burn', 'happy', 'servant', 'DINGO', 'immediately', 'Woa', 'alive', 'expect', 'Where', 'something', 'dress', 'told', 'vicious', 'mashed', 'would', 'Anybody', 'present', 'his', 'each', 'great', 'PRINCE', 'Hooray', 'finest', 'underwear', 'You', 'roar', 'badger', 'Since', 'leap', 'Oui', 'accent', 'perilous', 'escape', 'flint', 'haw', \"'\", 'along', 'which', 'sank', 'Shrubber', 'hee', 'wedding', 'shalt', 'quests', 'Mother', 'Oooh', 'lot', 'pointy', 'pack', 'Thppppt', 'bravest', 'ways', 'riding', 'Oooo', 'Alice', 'bastards', 'Surely', 'heh', 'may', 'wicked', 'chord', 'Burn', 'Eh', 'unsingable', 'va.', 'behold', 'carry', 'Hoo', 'relics', 'penalty', 'stop', 'Excuse', 'Aaaaaaaah', 'day..', 'SCENE', 'reasonable', 'Use', 'runes', 'model', 'tear', 'move', 'answers', 'home', 'Greetings', 'whinny', 'Old', 'hello', 'tonight', 'work', 'Huh', 'sixteen', 'Haw', 'Fetchez', 'autocracy', 'sheep', 'GUESTS', 'king-a', 'Camaaaaaargue', 'ham', 'triumphs', 'Saint', 'clang', 'Holy', 'miss', 'regulations', 'lose', 'bloody', 'ju', 'now', 'Ector', 'Bon', 'asks', 'biscuits', 'bridges', 'smelt', 'but', 'armor', 'afraid', 'groveling', 'SHRUBBER', 'twenty-four', 'Uugh', 'Anthrax', \"'Ecky-ecky-ecky-ecky-pikang-zoop-boing-goodem-zoo-owli-zhiv\", 'become', 'question', 'pram', 'decision', 'die', 'far', 'heads', 'temptress', 'GREEN', 'their', 'Huy', 'Ohh', 'None', 'people', 'perpetuates', 'bosom', 'sample', 'continue', 'Court', 'suddenly', 'arrows', 'CRAPPER', 'Knights', 'CHARACTER', 'else', 'aquatic', 'if', 'Forward', 'no', 'Look', 'master', 'until', 'punishment', 'VOICE', 'dead', 'Which', 'martin', 'VILLAGERS', 'Exactly', 'hacked', 'All', 'Nine', 'sex', 'Bravely', 'strand', 'live', 'chu', 'thanks', 'enter', 'did', 'forest', \"'Til\", 'Thee', 'g', 'African', 'finds', 'snore', 'basic', 'SECOND', 'treat', 'Explain', 'gained', 'accompanied', 'undressing', 'By', 'dance', 'excepting', 'sorry', 'clop', 'suppose', 'dangerous', 'hospitality', 'The', 'married', 'sink', 'unplugged', 'Aggh', 'capital', '3', 'already', 'stew', 'yelling', 'warmer', 'STUNNER', 'asking', 'music', 'stand', 'Every', 'false', 'called', 'Crapper', 'does', 'Psalms', 'folk', 'Excalibur', 'sign', 'attend', 'Aaaaaaaaah', 'prevent', 'non-migratory', 'goes', 'scott', 'ni', ',', 'Hah', 'guiding', 'doing', 'three', 'Three', 'aaaaaah', 'tops', 'Umm', 'course', 'thing', 'names', 'Get', 'nobody', 'line', 'on', 'design', 'uh', 'islands', 'newt', 'Mind', 'PATSY', 'Follow', 'sad', 'deal', 'do', 'held', 'reads', 'even', 'Found', 'Huyah', 'food', 'hiyaah', 'summon', 'stupid', 'Mercea', 'tinder', 'war', 'first', 'Castle', 'swamp', 'So', 'workers', 'Blue', 'oooh', 'buy', 'ill.', \"e'er\", 'know', 'special', 'running', 'shimmering', 'legs', 'Iiiives', 'centuries', 'donkey-bottom', 'trough', 'nine', 'agree', 'boys', '(', 'named', 'Launcelot', \"'em\", 'words', 'boom', 'fold', 'wayy', 'sawwwww', 'ROBIN', 'rabbit', 'search', 'carve', 'Chop', 'kneeling', 'ceremony', 'impersonate', 'Behold', 'Not', 'setting', 'tap-dancing', 'Even', 'footwork', 'sometimes', 'We', 'crying', 'guard', 'single-handed', 'strongest', 'warned', 'Arthur', 'GUEST', 'hast', 'j', 'GUARDS', 'rrrr', 'ponds', 'tiny', 'farcical', \"'round\", 'dine', 'be', 'Speak', 'pissing', 'angels', 'many', 'uuggggggh', 'clad', 'take', 'bond', 'reached', 'Well', 'classes', 'Go', 'intermission', 'Please', 'heroic', 'Never', 'lunged', 'Gallahad', 'Yeaah', 'If', 'Himself', 'nice', 'luck', 'large', 'orangutans', 'Robin', 'supports', 'than', 'electric', 'giggle', 'please', 'ooh', 'Leaving', 'wide', 'Dingo', 'mac', 'men', 'once', 'nervous', 'of', 'about', 'soiled', 'cadeau', 'Divine', 'gentle', 'humble', 'welcome', 'behind', 'terribly', 'worse', 'GALAHAD', 'B', 'Yup', 'order', 'Peril', 'individually', 'offensive', 'ARTHUR', 'that', 'door-opening', 'naughty', \"'Here\", 'Pin', 'seem', 'tart', 'meant', 'rock', 'Thy', 'aaugh', 'Would', 'this', 'persons', 'empty', 'Walk', 'HISTORIAN', 'bite', 'she', 'back', 'approacheth', 'passing', '20', 'breadth', 'There', 'path', ']', 'Bravest', 'whoever', 'lonely', 'pimples', 'we', 'GUARD', 'bathing', 'binding', 'able', 'etc', 'stress', 'training', 'carp', 'sort', 'nick', 'employed', 'Aaaaugh', 'purest', 'sword', 'whom', 'Y', 'Battle', 'face', 'sire', 'right', 'successful', 'dressing', 'Together', 'fooling', 'bad', 'known', 'OF', 'WOMAN', 'Bors', 'mayest', 'doors', 'CONCORDE', 'baaaa', 'looks', 'twong', 'MONKS', 'find', 'y', 'were', 'saying', 'dancing', 'forget', 'will', 'Actually', 'Pendragon', 'totally', 'beautiful', 'Ages', 'strangers', 'sovereign', 'all', 'speak', 'blanket', 'floats', 'ninepence', 'pen', 'Sorry', 'let', 'wishes', 'er', 'radio', 'weather', 'packing', 'Did', 'is', 'ENCHANTER', 'inside', '#', 'More', 'shrubbery', 'weighs', 'Clear', 'baby', 'committed', 'liar', 'legendary', 'enemies', 'spanking', 'Mine', 'k-nnniggets', \"'til\", 'not', 'Hyy', 'Ha', 'small', 'since', 'sniff', 'least', 'burned', 'haaa', 'dictatorship', 'threw', 'Bristol', 'whispering', \"'s\", 'True', 'stood', 'DENNIS', 'Chaste', 'n', 'kick', 'laurels', 'Hurry', 'Thank', 'scribble', 'alight', 'sell', 'spirit', 'country', 'husk', 'looked', 'became', 'auntie', 'l', 'there', 'as', 'kneecaps', \"'Man\", 'rather', 'taunt', 'particularly', 'Twenty-one', 'was', 'At', 'courage', 'bangin', 'separate', 's', 'castle', 'doubt', 'Providence', 'command', 'Hic', 'BROTHER', 'dub', 'big', 'animator', 'couple', 'Olfin', 'CUSTOMER', 'o', \"'m\", 'cough', 'cost', 'effect', 'try', 'vests', 'distributing', 'invincible', 'nineteen-and-a-half', 'RANDOM', 'seemed', 'peril', 'give-away', 'climes', 'dorsal', 'been', 'Beast', 'further', 'Lord', 'masses', 'He', 'Mud', 'guards', 'sneaking', 'BORS', 'these', 'bed', 'LOVELY', 'Alright', 'pweeng', 'shelter', 'thump', 'Dennis', 'unhealthy', \"'First\", 'frontal', 'run', 'Beyond', 'look', 'sworn', 'period', \"'sorry\", 'next', 'settles', 'blood', 'Therefore', 'found', 'Packing', 'considerable', 'color', 'tackle', 'birds', 'request', 'animal', 'Fine', 'Death', 'starling', 'two-level', 'confuse', 'accomplished', 'scarper', \"'To\", 'chops', 'system', 'kingdom', 'Neee-wom', 'Summer', 'refuse', 'ferocity', 'wrong', 'Autumn', 'Stand', 'Today', 'Bread', 'cart', 'ye', 'CART-MASTER', 'warning', 'split', 'hidden', 'after', 'air-speed', 'unladen', 'demand', 'Order', '11', 'scenes', 'somewhere', 'same', 'sequin', 'git', 'person', 'fled', 'handsome', 'BEDEVERE', 'out-clever', 'better', 'working', 'thank', 'walk', 'life', 'said', 'carrying', 'just', 'Iiiiives', 'strewn', 'suggesting', 'Be', 'carved', 'Nothing', 'keen', 'alarm', 'valleys', 'third', 'advancing', 'Rather', 'depart', 'duck', \"'forgive\", 'raped', 'curtains', 'requiem', 'its', 'unarmed', ';', 'Thsss', 'mooo', 'Aaagh', 'towards', 'them', 'lady', 'mean', 'suit', 'spam', 'thirty-seven', 'anarcho-syndicalist', 'counting', 'sigh', 'ANIMATOR', 'hospital', 'Over', 'pussy', 'given', 'appease', 'Patsy', 'actually', 'sod', 'repressing', 'listen', 'language', 'Quiet', 'nose', 'streak', 'valor', 'Dragon', 'lambs', 'count', 'To', 'laughing', 'women', 'earth', 'legally', 'aside', 'CARTOON', 'ridden', 'room', 'forced', 'crash', 'ride', 'Put', 'hoo', 'Ayy', 'Round', 'foul', 'understanding', 'Are', 'wan', 'Two', 'glad', 'foe', 'cast', 'felt', 'because', 'howl', 'expensive', 'waste', 'minutes', 'necessary', 'so-called', 'rich', \"'uuggggggh\", 'swords', 'Uhh', 'father', 'INSPECTOR', 'stretched', 'private', 'telling', 'Eee', 'arm', 'entered', 'and', 'Build', 'twin', 'thought', 'can', 'bridge', 'Defeat', 'where', 'leaps', 'thud', 'stops', 'put', 'land', 'gave', 'what', 'bats', 'Churches', 'kicked', 'worthy', 'Then', 'Uther', 'sweet', 'Yes', 'carving', 'the', 'day', 'sure', 'soon', 'knew', 'one', 'clack', 'anything', 'mile', '5', 'slightly', 'woosh', 'ignore', 'wound', 'THE', 'ehh', 'throughout', 'heeh', 'na', 'stay', 'indefatigable', 'noise', 'worried', 'bravely', 'guest', 'own', 'discovers', 'SENTRY', 'Angnor', 'making', 'Hee', \"'Ere\", 'taunting', 'Um', 'Peng', '24', 'Supreme', 'fell', 'quick', 'witches', 'defeator', \"n't\", 'by', 'Oooohoohohooo', 'Thpppppt', 'k-niggets', 'sponge', 'sent', 'gon', 'tropical', 'types', 'has', 'Listen', 'main', 'writing', 'sacrifice', \"'ll\", 'gay', 'bother', 'protect', 'through', 'types-a', 'silence', 'less', 'arrange', 'plan', 'conclusion', 'pay', 'again', 'draw', 'cheesy', 'away', '13', 'certain', 'icy', 'ratios', 'Guards', 'flight', 'Ahh', 'simple', 'at', 'limbs', 'gravy', 'particular'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2   Simple Topic Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter will introduce you to topic identification, which you can apply to any text you encounter in the wild. Using basic NLP models, you will identify topics from texts based on term frequencies. You'll experiment and compare two simple methods: bag-of-words and Tf-idf using NLTK, and a new library Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "file = open('datasets/Wikipedia articles/wiki_text_bug.txt', mode='r')\n",
    "\n",
    "# Print it\n",
    "# print(file.read())\n",
    "article = file.read()\n",
    "\n",
    "# Close file\n",
    "file.close()\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 295), ('the', 282), ('.', 220), ('a', 155), (\"''\", 155), ('of', 138), ('to', 131), ('``', 119), ('in', 103), ('{', 102)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tedeagan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tedeagan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bug', 126), ('software', 55), ('may', 45), ('computer', 42), ('code', 35), ('error', 33), ('program', 30), ('cite', 21), ('ref', 18), ('release', 18)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to gensim:\n",
    "### Creating and querying a corpus with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to apply the methods you learned in the previous video to create your first `gensim` dictionary and corpus!\n",
    "\n",
    "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called `articles`. You'll need to do some light preprocessing and then generate the `gensim` dictionary and corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary \n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf with Gensim:\n",
    "\n",
    "### Tf-idf with Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to determine new significant terms for your corpus by applying `gensim`'s tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises - `dictionary`, `corpus`, and `doc`. Will tf-idf make for more interesting results on the document level?\n",
    "\n",
    "`TfidfModel` has been imported for you from `gensim.models.tfidfmodel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Named-entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Building a \"fake news\" classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building word count vectors with scikit-learn:\n",
    "\n",
    "### CountVectorizer for text classification\n",
    "\n",
    "It's time to begin building your text classifier! The data has been loaded into a DataFrame called `df`. Explore it in the IPython Shell to investigate what columns you can use. The `.head()` method is particularly informative.\n",
    "\n",
    "In this exercise, you'll use `pandas` alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a `CountVectorizer` and investigate some of its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/fake_or_real_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillary’s Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n"
     ]
    }
   ],
   "source": [
    "# Print the head of df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillary’s Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n",
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33,random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer for text classification\n",
    "\n",
    "Similar to the sparse `CountVectorizer` created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a `TfidfVectorizer` and investigate some of its features.\n",
    "\n",
    "In this exercise, you'll use pandas and `sklearn` along with the same `X_train`, `y_train` and `X_test`, `y_test` DataFrames and Series you created in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the vectors\n",
    "\n",
    "To get a better idea of how the vectors work, you'll investigate them by converting them into `pandas` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "\n",
      "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "\n",
      "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing a classification model with scikit-learn\n",
    "\n",
    "### Text classification models:\n",
    "\n",
    "Naive Bayes is the most reasonable model to use when training a new supervised model using text vector data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with CountVectorizer\n",
    "\n",
    "Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the `CountVectorizer` data.\n",
    "\n",
    "The training and test sets have been created, and `count_vectorizer`, `count_train`, and `count_test` have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893352462936394\n",
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm =  metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with TfidfVectorizer\n",
    "\n",
    "Now that you have evaluated the model using the `CountVectorizer`, you'll do the same using the `TfidfVectorizer` with a Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm =  metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NLP, complex problems:\n",
    "\n",
    "### Improving your model\n",
    "\n",
    "Your job in this exercise is to test a few different alpha levels using the `Tfidf` vectors to determine if there is a better performing combination.\n",
    "\n",
    "The training and test sets have been created, and `tfidf_vectorizer`, `tfidf_train`, and `tfidf_test` have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8813964610234337\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8589191774270684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting your model\n",
    "\n",
    "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.\n",
    "\n",
    "You have your well performing tfidf Naive Bayes classifier available as `nb_classifier`, and the vectors as `tfidf_vectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
      "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
